# Image Captioning 

## Overview

This project implements an **Image Captioning System** using a **sequence-to-sequence model** with **LSTM** (Long Short-Term Memory) networks to generate contextually relevant captions for images. **VGG16** is used for feature extraction, converting images into feature vectors that are then fed into the LSTM network for caption generation. The model was trained on the **Flickr8k dataset**, which contains 8,000 images and their associated captions. The goal of this project is to produce descriptive captions for images. The project demonstrates the application of deep learning techniques to solve real-world problems in the field of assistive technologies.


